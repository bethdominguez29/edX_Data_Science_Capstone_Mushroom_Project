---
title: "edX Data Science: Capstone Course - Prediction of Mushroom Edibility Project"
subtitle: "If a mushroom smells bad - don't eat it!"
author: "Elizabeth Plasse Dominguez"
date: "October 11, 2022"
output:
  pdf_document
---

## Overview

The goal of this Capstone Course - Prediction of Mushroom Edibility Project was to create two different machine learning models from the same mushroom data that predict whether a mushroom is edible or not and to evaluate the accuracy, sensitivity and specificity of prediction of the models.

The mushroom data for this project was the UC Irvine (UCI) Machine Learning Repository Mushroom Dataset^1^ and comprised 23 categorical variables. "This data set includes descriptions of ... 23 species of gilled mushrooms in the Agaricus and Lepiota Family \..."^2^. Globally, there are over 14,000 species of mushrooms with about 13,000 gilled mushroom species so this mushroom data set was a small sample of the population.

Predicting the edibility of mushrooms is not easy. People who forage for mushrooms acquire a specific knowledge base in order to successfully gather edible mushrooms. They also generally use the rule "When in doubt throw it out!" with regard to edibility. Additionally, the UCI website page for the mushroom data cautions "The Guide [... Audubon Society Field Guide to ... Mushrooms] clearly states that there is no simple rule for determining the edibility of a mushroom ..."^2^.

In light of these warnings and the small sample size, the prediction models resulting from this data should not be considered applicable to mushrooms in general and mushrooms should only be consumed when their edibility is known with certainty.

The data was downloaded, cleaned and manipulated into data frame training and test sets and one-hot coded matrix training and test sets.

Data investigation, analysis and visualization of the training set data, detailed in the Methods section, led to setting "edible" as the positive class and to the training of two prediction models - a regularized logistic regression model and a random forest classification model. The trained models were used to make predictions from the test set data. Due to the serious consequences of eating a poisonous mushroom the typical rule for classifying prediction probabilities as 1 for probabilities over 0.5 was made stricter by requiring prediction probabilities to be greater than 0.99 in order to be set equal to 1.

Both models predicted the edibility of mushrooms in the test data with high accuracy. The random forest model predicted with 100% accuracy. Sensitivity and specificity also both equaled 1. The regularized logistic model had an accuracy of 99.9%, a sensitivity of 1, a specificity of 0.997 and misclassified two edible observations as non-edible (false negatives). Perhaps most important neither model classified non-edible mushrooms as edible - no false positives. See the following summary.

\pagebreak

|                      |          |             |             |                 |                 |
|:-----------|:----------:|:----------:|:----------:|:----------:|:----------:|
| Models               | Accuracy | Sensitivity | Specificity | False Positives | False Negatives |
| Regularized Logistic |  0.999   |    1.000    |    0.997    |        0        |        2        |
| Random Forest        |  1.000   |    1.000    |    1.000    |        0        |        0        |

## Methods

### Data Exploration and Cleaning

The mushroom data download from the UCI website had no header. Column names and factor levels for all variables were provided on the UCI website page. See **Appendix** at end of this report for a list of variables and their levels (unused levels are not reported).

Data from the UCI Mushrooms data set was downloaded into a data frame object named *all_mushroom_dat*, column names were added, and all variables were made into factors. Here is a summary of the 8124 rows and 23 columns of *all_mushroom_dat*.

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment = NA}
################################################################
# download data and create new data frame
################################################################

# install tidyverse, caret, and data.table packages if needed
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

#call needed packages
library(tidyverse)
library(caret)
library(data.table)

# mushroom dataset link:
# https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data

# create object of data url - mushroom_url
mushroom_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data"

# create temporary file for mushroom data
temp_filename <- tempfile()

# download csv mushroom data into temporary file
download.file(mushroom_url, temp_filename)

# create r df object containing all mushroom data
all_mushroom_dat <- as.data.frame(read.csv(temp_filename,
                                           header = FALSE,
                                           sep = ","))

# create vector of column names
column_names <- c("is_edible", "cap_shape", "cap_surface",
                  "cap_color","does_it_bruise", "odor",
                  "gill_attachment", "gill_spacing",
                  "gill_size", "gill_color", "stalk_shape",
                  "stalk_root",
                  "stalk_surface_above_ring",
                  "stalk_surface_below_ring",
                  "stalk_color_above_ring",
                  "stalk_color_below_ring",
                  "veil_type", "veil_color", "ring_number",
                  "ring_type","spore_print_color", "population",
                  "habitat")

# set column names
colnames(all_mushroom_dat) <- column_names

# make variables' classes equal to factor
make_factors <- c(1:23)
all_mushroom_dat[,make_factors] <- lapply(all_mushroom_dat[,make_factors], factor)

# remove unneeded objects
rm(mushroom_url, temp_filename, make_factors)

# look at structure of data frame
str(all_mushroom_dat)
```

Initial exploration of *all_mushroom_dat* showed the data to be quite clean. It contained no NAs and no empty cells. Two of the independent variables, *veil_type* and *stalk_root*, however, stood out.

*veil_type* had one level with 8124 observations of "p". This variable had no variability so *veil_type* was eliminated from the data.

*stalk_root* had 5 levels with one marked "?" which contained 2480 missing observations. Elimination of this missing data was accomplished by removing the *stalk_root* column rather than the 2480 observation rows. This minimized the loss of data for analysis and the potential of skewing the sample by removing data from other independent variables.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
################################################################
# create new data frame of mushroom data without veil_type and stalk_root columns
################################################################

mushroom_dat <- all_mushroom_dat %>%
  select(-veil_type, -stalk_root)
```

The elimination of these two independent variables lead to creation of a data frame object *mushroom_dat.* This data frame contained `r nrow(mushroom_dat)` rows and `r ncol(mushroom_dat)` columns. The dependent variable, *is_edible*, was a binary factor and was in the first column of this data frame. The 20 remaining independent variables were all factors and were in columns 2 - 21. The levels of the factors ranged from 2 to 12.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
################################################################
# create mushroom_training_set and mushroom_test_set
################################################################

# make mushroom_test_set 20% of mushroom_dat
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = mushroom_dat$is_edible,
                                  times = 1, p = 0.2,
                                  list = FALSE)
mushroom_training_set <- mushroom_dat[-test_index,]
mushroom_test_set <- mushroom_dat[test_index,]
```

*mushroom_dat* was randomly partitioned into training (80%) and test (20%) sets.

|                         |                                 |                                 |
|:-------------------:|:-----------------------:|:------------------------:|
|       Data Frame        |         Number of Rows          |        Number of Columns        |
| *mushroom_training_set* | `r nrow(mushroom_training_set)` | `r ncol(mushroom_training_set)` |
|   *mushroom_test_set*   |   `r nrow(mushroom_test_set)`   |   `r ncol(mushroom_test_set)`   |

The training and test sets contained:

-   the dependent variable *is_edible,* and
-   20 independent variables: *cap_shape, cap_surface, cap_color, does_it_bruise, odor, gill_attachment, gill_spacing, gill_size, gill_color, stalk_shape, stalk_surface_above_ring, stalk_surface_below_ring, stalk_color_above_ring, stalk_color_below_ring, veil_color, ring_number, ring_type, spore_print_color, population, habitat.*

The following summary of the distribution of classes of the dependent variable in the training and test sets shows they are quite similar.

```{r, include=FALSE}
#################################################################
# code for proportions of dependent variable in training and test sets
#################################################################

options(digits = 3)
# proportion of edible (e) vs non-edible (p) in training set
sum(mushroom_training_set$is_edible == "e") # [1] 3366
sum(mushroom_training_set$is_edible == "e")/
  nrow(mushroom_training_set) # [1] 0.518
sum(mushroom_training_set$is_edible == "p") # [1] 3132
sum(mushroom_training_set$is_edible == "p")/
  nrow(mushroom_training_set) # [1] 0.482

# proportion of edible (e) vs non-edible (p) in test set
sum(mushroom_test_set$is_edible == "e") # [1] 842
sum(mushroom_test_set$is_edible == "e")/
  nrow(mushroom_test_set) # [1] 0.518
sum(mushroom_test_set$is_edible == "p") # [1] 784
sum(mushroom_test_set$is_edible == "p")/
  nrow(mushroom_test_set) # [1] 0.482
```

| Dependent Variable (*is_edible*) Factor Levels |                            Percent of Dependent Variable in Training Set                            |                          Percent of Dependent Variable in Test Set                          |
|:-----------------:|:-------------------------:|:------------------------:|
|                   e (edible)                   | `r round(((sum(mushroom_training_set$is_edible == "e")/   nrow(mushroom_training_set)) * 100), 1)`% | `r round(((sum(mushroom_test_set$is_edible == "e")/   nrow(mushroom_test_set)) * 100), 1)`% |
|                 p (non-edible)                 | `r round(((sum(mushroom_training_set$is_edible == "p")/   nrow(mushroom_training_set)) * 100), 1)`% | `r round(((sum(mushroom_test_set$is_edible == "p")/   nrow(mushroom_test_set)) * 100), 1)`% |

### Analysis and Visualization

Plots showing the breakdown of edible and non-edible observations by factor level for each of the 20 independent variables in *mushroom_training_set* are presented below in Figure 1.

These plots provided insight into which of the independent variables may be important predictors. The Odor, Spore Print Color, Gill Color and Population variables stand out as potential good predictors. These four features have some levels with only edible or only non-edible observations and also have other levels with a majority of observations in one class. The Odor variable in particular seems to predict edibility well as observations of the dependent variable in all levels except one (the level "none") were either edible or non-edible.

Alternatively, the plots of the Cap Surface, Gill Attachment, Stalk Shape, Veil Color and Ring Number variables show that they may not be good predictors. Their breakdowns of observations by levels are close to half edible and half non-edible and are similar to just guessing or flipping a coin.

In addition the plots of the Stalk Surface Above Ring and Stalk Surface Below Ring variables look very similar to each other as do the plots of the Stalk Color Above Ring and Stalk Color Below Ring variables. This suggests there is a high degree of collinearity between these pairs of variables.

\pagebreak

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#################################################################
# Figure 1 plot observations of dependent variable levels e and p for each independent variables by levels
#################################################################

# install cowplot and ggpubr packages if needed
if(!require(tidyverse)) install.packages("cowplot", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("ggpubr", repos = "http://cran.us.r-project.org")

# call needed packages
library(cowplot)
library(ggpubr)

#################################################################
# create plot of individual independent variables
#################################################################
# plots will be in grid of 4 rows and 3 columns to a page  
# plot in first row and first column of page 1 need legend and y axis label
# plots in first column of pages 1 and 2 of figure need y axis label but no legend
# plots in second and third columns of pages 1 and 2 do not need legend or y axis label

################################################################
# create plot for cap_shape versus is_edible
# include legend and y axis label
cap_shape_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(cap_shape, fill = factor(is_edible))) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("bell",
                              "conical",
                              "flat",
                              "knobbed",
                              "sunken",
                              "convex")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Cap Shape",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1),
        axis.title.y = element_text(size = 10)) +
  theme(legend.title = element_text(size = 9),
        legend.key.size = unit(0.2, "cm"))

###############################################################
# create plot for cap_surface versus is_edible
# don't include legend or y axis label
cap_surface_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(cap_surface, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("fibrous",
                              "grooves",
                              "smooth",
                              "scaly")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Cap Surface",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for cap_color versus is_edible
# don't include legend or y axis label
cap_color_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(cap_color, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("buff",
                              "cinnamon",
                              "red",
                              "gray",
                              "brown",
                              "pink",
                              "green",
                              "purple",
                              "white",
                              "yellow")) +
 scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Cap Color",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for does_it_bruise versus is_edible
# include y axis label but not legend
does_it_bruise_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(does_it_bruise, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("false",
                              "true")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Does It Bruise",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1),
        axis.title.y = element_text(size = 10))

################################################################
# create plot for odor versus is_edible
# don't include legend or y axis label
odor_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(odor, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("almond",
                              "creosote",
                              "foul",
                              "anise",
                              "musty",
                              "none",
                              "pungent",
                              "spicy",
                              "fishy")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Odor",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for gill_attachment versus is_edible
# don't include legend or y axis label
gill_attachment_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(gill_attachment, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("attached",
                              "free")) +
 scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Gill Attachment",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for gill_spacing versus is_edible
# include y axis label but not legend
gill_spacing_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(gill_spacing, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("close",
                              "crowded")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Gill Spacing",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1),
        axis.title.y = element_text(size = 10))

################################################################
# create plot for gill_size versus is_edible
# don't include legend or y axis label
gill_size_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(gill_size, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("broad",
                              "narrow")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Gill Size",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for gill_color versus is_edible
# don't include legend or y axis label
gill_color_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(gill_color, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("buff",
                              "red",
                              "gray",
                              "chocolate",
                              "black",
                              "brown",
                              "orange",
                              "pink",
                              "green",
                              "purple",
                              "white",
                              "yellow")) +
 scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Gill Color",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

###############################################################
# create plot for stalk_shape versus is_edible
# include y axis label but not legend
stalk_shape_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(stalk_shape, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("enlarging",
                              "tapering")) +
scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Stalk Shape",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1),
        axis.title.y = element_text(size = 10))

##############################################################
# create plot for stalk_surface_above_ring versus is_edible
# don't include legend or y axis label
stalk_surface_above_ring_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(stalk_surface_above_ring, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("fibrous",
                              "silky",
                              "smooth",
                              "scaly")) +
 scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Stalk Surface Above Ring",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

###############################################################
# create plot for stalk_surface_below_ring versus is_edible
# don't include legend or y axis label
stalk_surface_below_ring_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(stalk_surface_below_ring, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("fibrous",
                              "silky",
                              "smooth",
                              "scaly")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Stalk Surface Below Ring",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for stalk_color_above_ring versus is_edible
# include y axis label but not legend
stalk_color_above_ring_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(stalk_color_above_ring, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("buff",
                              "cinnamon",
                              "red",
                              "gray",
                              "brown",
                              "orange",
                              "pink",
                              "white",
                              "yellow")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Stalk Color Above Ring",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1),
        axis.title.y = element_text(size = 10))

##################################################################
# create plot for stalk_color_below_ring versus is_edible
# don't include legend or y axis label
stalk_color_below_ring_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(stalk_color_below_ring, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("buff",
                              "cinnamon",
                              "red",
                              "gray",
                              "brown",
                              "orange",
                              "pink",
                              "white",
                              "yellow")) +
scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Stalk Color Below Ring",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for veil_color versus is_edible
# don't include legend or y axis label
veil_color_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(veil_color, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("brown",
                              "orange",
                              "white",
                              "yellow")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Veil Color",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for ring_number versus is_edible
# include y axis label but not legend
ring_number_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(ring_number, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("none",
                              "one",
                              "two")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Ring Number",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1),
        axis.title.y = element_text(size = 10))

################################################################
# create plot for ring_type versus is_edible
# don't include legend or y axis label
ring_type_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(ring_type, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("evanescent",
                              "flaring",
                              "large",
                              "none",
                              "pendant")) +
 scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Ring Type",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for spore_print_color versus is_edible
# don't include legend or y axis label
spore_print_color_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(spore_print_color, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("buff",
                              "chocolate",
                              "black",
                              "brown",
                              "orange",
                              "green",
                              "purple",
                              "white",
                              "yellow")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Spore Print Color",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))

#################################################################
# create plot for population versus is_edible
# include y axis label but not legend
population_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(population, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("abundant",
                              "clustered",
                              "numerous",
                              "scattered",
                              "several",
                              "solitary")) +
  scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Population",
       y = "Number of Observations",
       fill = "edible") +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1),
        axis.title.y = element_text(size = 10))

#################################################################
# create plot for habitat versus is_edible
# don't include legend or y axis label
habitat_plot <- ggplot(data = mushroom_training_set) +
  geom_bar(aes(habitat, fill = factor(is_edible)),
           show.legend = FALSE) +
  scale_y_continuous(limits = c(0, 6500),
                     labels = scales::comma) +
  scale_x_discrete(labels = c("woods",
                              "grasses",
                              "leaves",
                              "meadows",
                              "paths",
                              "urban",
                              "waste")) +
 scale_fill_manual(values = c("green", "red"),
                    labels = c("yes", "no")) +
  labs(x = "Habitat",
       fill = "edible") +
  theme(axis.title.y = element_blank()) +
  theme(axis.title.x = element_text(size = 10),
        axis.text.x = element_text(angle = 45,
                                   vjust = 1,
                                   hjust = 1))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
################################################################
# create first page of Figure 1
# Figure 1: Observations of Dependent Variables by Levels of Independent Variables
################################################################

# need cap_shape_plot, cap_surface_plot, cap_color_plot, does_it_bruise_plot, odor_plot, gill_attachment_plot

group_1 <- ggarrange(cap_shape_plot,
          cap_surface_plot,
          cap_color_plot,
          does_it_bruise_plot,
          odor_plot,
          gill_attachment_plot,
          ncol = 3,
          nrow = 2,
          common.legend = TRUE,
          legend = "top")

# set title for page one Figure 1
annotate_figure(group_1,
                top = text_grob("Figure 1: Observations of Dependent Variable \nby Levels of Independent Variables",
                                color = "black",
                                face = "bold",
                                size = 12))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#################################################################
# fill in next six plots on page 1 of Figure 1
#################################################################

# need gill_spacing_plot, gill_size_plot, gill_color_plot, stalk_shape_plot, stalk_surface_above_ring_plot, stalk_surface_below_ring_plot

ggarrange(gill_spacing_plot,
          gill_size_plot,
          gill_color_plot,
          stalk_shape_plot,
          stalk_surface_above_ring_plot,
          stalk_surface_below_ring_plot,
          ncol = 3,
          nrow = 2)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#################################################################
# create second page of Figure 1
# Figure 1 Continued: Observations of Dependent Variables by Levels of Independent Variables
################################################################
# need stalk_color_above_ring_plot, stalk_color_below_ring_plot, veil_color_plot, ring_number_plot, ring_type_plot, spore_print_color_plot

group_2 <- ggarrange(stalk_color_above_ring_plot,
          stalk_color_below_ring_plot,
          veil_color_plot,
          ring_number_plot,
          ring_type_plot,
          spore_print_color_plot,
          ncol = 3,
          nrow = 2)

# set title for page two of Figure 1
annotate_figure(group_2,
                top = text_grob("Figure 1 Continued: Observations of Dependent Variable \nby Levels of Independent Variables",
                                color = "black",
                                face = "bold",
                                size = 12))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
################################################################
# fill in final 2 plots on page 2 of Figure 1
################################################################

# need population_plot, habitat_plot

ggarrange(population_plot,
          habitat_plot,
          ncol = 3,
          nrow = 2)
```

\pagebreak

Cramér's V^3^ was chosen to investigate the level of association between the independent variables in *mushroom_training_set.* Cramér's V is a statistic that ranges from 0 to 1 and measures the association between two nominal^4^ variables. Judging the level of association characterized by Cramér's V depends upon the number of degrees of freedom that exist in the comparison.

For this project the Cramér's V calculations between independent variables had 1 degree of freedom which was determined by taking the minimum of the number of rows minus one (6498 - 1 = 6497) and the number of columns minus one (2 -1 = 1). With one degree of freedom, pairs of independent variables with Cramér's Vs of 0.5 and above have medium to strong associations, while those with lower results have small associations.

There is plenty of evidence of medium to strong association between the independent variables in the training set data as is shown in the Figure 2 below. In fact `r (18/20) * 100`% of the independent variables had medium to strong associations. *cap_color*, *gill_spacing*, and *stalk_surface_above_ring* had medium to strong of association with two other independent variables while *ring_type* had it with eleven other independent variables. *gill_attachment*, *stalk_color_above_ring*, *stalk_color_below_ring*, and *veil_color* all have very strong associations with other independent variables.

Additionally, Figure 2 shows a strong relationship between *stalk_color_above_ring* and *stalk_color_below_ring* which showed very similar plots of breakdown of dependent variable observations by independent variable level in Figure 1. *stalk_surface_above_ring* and *stalk_surface_below_ring* also had very similar plots of breakdown of dependent variable observations by independent variable level in Figure 1 however they do not appear on the Cramér's V plot because their Cramér's V of 0.49 is just below the 0.5 cut off.

Two independent variables, *cap_shape* and *cap_surface*, had Cramér's Vs with the other independent variables that were all below 0.5 and thus small association with them.

Figure 2 shows the medium to strong associations in the independent variables with a color scale changing from orange to brown as the Cramér's V increases from 0.5 to just under 1. The diagonal of Cramér's V equal to 1 for a variable with itself has been omitted from the plot. Note also the two blank columns on the far left for the two independent variables with Cramér's Vs below the cutoff for the plot.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
##################################################################
# create Figure 2
#   Figure 2: Association Between Independent Variables Cramér's V of 0.5 or Greater
##################################################################

# plot of Cramér's V >= 0.5 for all independent variables

# install lsr package if needed
if(!require(lsr)) install.packages("lsr", repos = "http://cran.us.r-project.org")
# install corrplot package if needed
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
# install DescTools package if needed
if(!require(DescTools)) install.packages("DescTools", repos = "http://cran.us.r-project.org")

# lsr package contains cramersV() function
# corrplot package contains corrplot() function
# DescTools package contains PairApply() function

#call needed packages
library(lsr)
library(corrplot)
library(DescTools)

# create matrix of Cramér's V for all independent variables in training set
cramV_training <- as.data.frame(PairApply(mushroom_training_set[,-1],
                                          cramersV))

# change all Cramér's V < 5 to 0 in cramV_training
cramV_training[cramV_training < 0.5] <- NA

# set color palette for plot
col <- colorRampPalette(c("orange", "brown"))

# plot Cramér's V >= 0.5 for independent variables with diagonal removed 
corrplot(as.matrix(cramV_training),
         method = "color",
         type = "lower",
         title = "Figure 2: Association Between Independent Variables \nCramér's V of 0.5 or greater",
         is.corr = FALSE,
         diag = FALSE,
         mar = c(0, 0, 2, 0),
         addgrid.col = "grey",
         tl.col = "black",
         tl.cex = 0.8,
         tl.srt = 45,
         col = col(100),
         na.label.col = "white",
         cl.cex = 0.7)
```

\pagebreak

### Approach and Design for Models

Since *mushroom_training_set* has a binary dependent variable with only two classes and nominal categorical independent variables, logistic regression and random forest algorithms were chosen as the two models for training. Both of these machine learning methods perform well when predictioning with categorical data.

The data frames *mushroom_training_set* and *mushroom_test_set* which contain 21 categorical variables as factors were used for training and prediction ability testing the random forest model.

The matrices *x_train_glmnet*, *y_train_glmnet*, *x_test_glmnet*, and *y_test_glmnet* were used for training and prediction testing the regularized logisitc model. These matrices were created by one-hot encoding the *mushroom_training_set* and *mushroom_test_set* data frames and then manipulating the placement of the dependent variable. *x_train_glmnet* and *x_test_glmnet* contain the resulting 111 one-hot coded features created from the 20 independent variables in *mushroom_training_set* and *mushroom_test_set*. *y_train_glmnet* and *y_test_glmnet* contain the one-hot coded dependent variable created from the dependent variable in *mushroom_training_set* and *mushroom_test_set*.

The positive class for model training and prediction was chosen to be "edible".

**Logistic Model**

Logistic regression is a linear regression of the log odds of the dependent variable and is used to model a binary response variable. The dependent variable of *mushroom_training_set*, $y_{i}$ for the i-th observation, is categorical and binary with its positive class equal to "e" for edible. The independent variables of *mushroom_training_set* in the vector $X_{i}$ are all nominal and categorical.

A logistic model finds coefficients (the betas), that produce probabilities of $y_i$ = 1 for given predictor variables $x_i$, which do the best job of accurately classifying the observed data points. The log odds transformation is used to assure that the output probabilities will be between 0 and 1. Additionally, logistic regression uses the log likelihood ($L$) to estimate the coefficients and probability distribution (via maximum likelihood estimation) that best explain the data.

The logistic model is defined by:

$$logit(p(X_{i})) = log(\frac{p(X_{i})}{1-p(X_{i})}) = \beta_{0} + \beta_{1}x_{i1} + ... + \beta_{k}x_{ik}$$where:

$X_{i} = (x_{i1}, ... , x_{ik})^T$ are the $k$ predictor variables for the $i$-th observation,

$p(X_{i})$ is the probability that $y_{i}$ equals 1, given $X_{i}$, and

$\beta_{0}, ..., \beta_{k}$ are the model coefficients.

The model minimizes the log likelihood function over the coefficients.

$$L = -log(\prod_{i:y_i=+}p(X_{i}) \prod_{j:y_j=-}(1 - p(X_{j})))$$

where:

$L$ is the log likelihood function

$\prod_{i:y_{i}=+}$ is the product over $i$ for the positive group of $y_{i}$,

$\prod_{j:y_{j}=-}$is the product over $j$ for the non-positive group of $y_{j}$.

Training of *mushroom_training_set* on this logistic regression model resulted in a model that did not converge and had singularities. These are signs of the multicollinearity and potential perfect separation in the independent variables as seen in the Analysis and Visualization section Figure 2.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
##################################################################
# train logistic regression model with mushroom_training_set
##################################################################

logistic_model <- glm(is_edible ~ .,
                   family = binomial(link = "logit"),
                   data = mushroom_training_set)

# Warning messages:
# 1: glm.fit: algorithm did not converge 
# 2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
```

\pagebreak

Logistic regression with LASSO (least absolute shrinkage and selection operator) regularization can be helpful to produce a model that adjusts for multicollinearity and perfect separation. The regularization introduces bias while reducing variance. LASSO regularization adds a penalty term to the log likelihood function of the logistic regression model. This penalty is intended to reduce the coefficients of less significant independent variables to zero, reducing the effect of multicollinearity. It uses this log likelihood with LASSO penalty function.

$$L + \lambda(|\beta_{0}| + |\beta_{1}| + |\beta_{2}| + ... + |\beta_{k}|)$$

where:

$L$ is the log likelihood function,

$\lambda$ is a model parameter selected to minimize the out of sample error of the model, and

$\beta_{0}, ..., \beta_{k}$ are the model coefficients.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#################################################################
# create training and test sets to train regularized logistic model
#################################################################

# install glmnet package if needed
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org")

# install mltools package if needed
if(!require(mltools)) install.packages("mltools", repos = "http://cran.us.r-project.org")

#call glmnet and mtools packages
library(glmnet)
library(mltools)

#################################################################
# need mushroom_training_set in one-hot coded matrix for glmnet()
#################################################################

# first change mushroom_training_set data frame into data table
#   for use in one_hot() function  
dtemp_training <- as.data.table(mushroom_training_set)

# now run one-hot() to create oh_mushroom_training_set
oh_mushroom_training_set <- one_hot(dtemp_training)

# need mushroom_test_set in one-hot coded matrix for glmnet()
# first change mushroom_test_set data frame into data table
#   for use in one_hot() function  
dtemp_test <- as.data.table(mushroom_test_set)

# now run one-hot() to create oh_mushroom_test_set
oh_mushroom_test_set <- one_hot(dtemp_test)

# remove unneeded objects
rm(dtemp_training, dtemp_test)

##############################################################
# create x_train_glmnet matrix and y_train_glmnet matrices for
#   model training - glmnet requires for family = "binomial"
##############################################################

# create x_train_glmnet matrix
x_train_glmnet <- as.matrix(oh_mushroom_training_set)[,-1:-2]
# create data frame to get dimensions for use in inline code
df_x_train_glmnet <- data.frame(rows = nrow(x_train_glmnet),
                                columns = ncol(x_train_glmnet))

# create y_train_glmnet matrix
# make is_edible_e second as second column is target class
y_train_glmnet <- as.matrix(oh_mushroom_training_set)[, 2:1]
# create data frame to get dimensions for use in inline code
df_y_train_glmnet <- data.frame(rows = nrow(y_train_glmnet),
                                columns = ncol(y_train_glmnet))

###########################################################
# create x_test_glmnet and y_test_glmnet matrices for
#   model testing - glmnet requires for family = "binomial"
###########################################################

# create x_test_glmnet matrix
x_test_glmnet <- as.matrix(oh_mushroom_test_set)[,-1:-2]
# create data frame to get dimensions for use in inline code
df_x_test_glmnet <- data.frame(rows = nrow(x_test_glmnet),
                                columns = ncol(x_test_glmnet))

# create y_test_glmnet matrix
# make is_edible_e second as second column is target class
y_test_glmnet <- as.matrix(oh_mushroom_test_set)[, 2:1]
# create data frame to get dimensions for use in inline code
df_y_test_glmnet <- data.frame(rows = nrow(y_test_glmnet),
                                columns = ncol(y_test_glmnet))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
###############################################################
# train regularized logistic model with cross validation to determine optimal lambda (lambda.min) to minimize out of sample error
###############################################################

# train model x_train_glmnet and y_train_glmnet with:
#   family = "binomial", and
#   alpha = 1 (LASSO regularization)
# set.seed() for reproducibility as results from cv.glmnet are random
set.seed(2, sample.kind="Rounding")
cv_LASSO_model <- cv.glmnet(x_train_glmnet,
                               y_train_glmnet,
                               family = "binomial",
                               alpha = 1)
#############################################################

# create data frame of coefficients of regularized model using result of lambda.min from the cross validation
cv_lasso_coef_min_lambda <- as.data.frame(as.matrix(coef(cv_LASSO_model,
                           s = min(cv_LASSO_model$lambda))))

# create data frame of non_zero coefficients by naming coefficients column, filtering for non-zero coefficients, rounding coefficients to 1 significant digit and selecting them
non_zero_cv_lasso_coefficients <-cv_lasso_coef_min_lambda %>% 
  mutate(CV_LASSO_Coefficients = s1) %>% 
  filter(CV_LASSO_Coefficients != 0) %>% 
  mutate(CV_LASSO_Coefficients = round(CV_LASSO_Coefficients, 1)) %>% 
  select(CV_LASSO_Coefficients)
```

Training for a LASSO model required the training data to be in matrix format so *mushroom_training_set* and *mushroom_test_set* were one-hot encoded and these matrices were created:

|                  |                            |                            |
|:----------------:|:--------------------------:|:--------------------------:|
|      Matrix      |       Number of Rows       |     Number of Columns      |
| *x_train_glmnet* | `r df_x_train_glmnet[1,1]` | `r df_x_train_glmnet[1,2]` |
| *y_train_glmnet* | `r df_y_train_glmnet[1,1]` | `r df_y_train_glmnet[1,2]` |
| *x_test_glmnet*  | `r df_x_test_glmnet[1,1]`  | `r df_x_test_glmnet[1,2]`  |
| *y_test_glmnet*  | `r df_y_test_glmnet[1,1]`  | `r df_y_test_glmnet[1,2]`  |

Additionally, cross validation was implemented to determine the best $\lambda$ to use with the CV LASSO model when making predictions in order to minimized the out of sample error.

CV LASSO model training on *x_train_glmnet* and *y_train_glmnet* and cross validation was performed using the cv.glmnet function with:

-   "family" set to "binomial", and
-   "alpha" set to 1 (calls LASSO regularization).

\pagebreak

The trained CV LASSO model converged with no singularities and found coefficients for `r length(non_zero_cv_lasso_coefficients$CV_LASSO_Coefficients) - 1` of the `r ncol(x_train_glmnet)` one-hot coded independent variables. Here are the model selected variables with fitted log odds coefficients (including intercept):

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment = NA}
################################################################
# print non-zero coefficients
################################################################

non_zero_cv_lasso_coefficients
```

The signs of these coefficients match well with the plots of the independent variables in the Analysis and Visualization section Figure 1. For example, the odor variable coefficients for odor_a and odor_l are positive and their columns in the plot of the Odor variable show only edible observations. While the coefficient for odor_f is negative and its column in the plot of the Odor variable show only non-edible observations. It is interesting to note that large coefficients for odor_a, odor_f, odor_l, odor_n, spore_print_color_r, and population_c seem to agree with intuition from the Analysis and Visualization section that Odor, Spore Print Color and Population variables might be good predictors. However no work was done to determine the significance of these coefficients.

The model also produced a very low mean cross-validated error (binomial deviance) of `r min(cv_LASSO_model$cvm)` which is a sign that the model fits the data well.

CV LASSO model prediction was performed using the predict function with:

-   *cv_lasso_model*,
-   *x_test_glmnet* (the test data features),
-   "s" set to "lambda.min",
-   "type" set to "response".

Lambda.min determined by the cross validation was used to produced probabilities for all observations. The strict rule of using 0.99 as the hurdle for positive prediction was implemented and probabilities were then converted to predictions of 1 if they were higher than 0.99 or 0 if they were not. These predictions were then compared to *y_test_glmnet* (test data results) and confusion matrix statistics were calculated.

\pagebreak

**Random Forest Model**

Random forest is a nonlinear ensemble machine learning method for classification/prediction of binary categorical variables which:

-   takes bootstrapped (random with replacement) samples from the training data and builds a tree for each sample,
-   considers a random subset of the independent variables at each node of each decision tree,
-   constructs many decision trees from the training data,
-   considers the class of votes, positive or negative, from all trees for each observation, and
-   selects the output class for each observation based upon which class has the majority of the considered votes from all trees.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#################################################################
#train random forest model
#################################################################

# install randomForest package if needed
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")

# call randomForest package
library(randomForest)

################################################################

# train model with:
#   mushroom_training_set,
#   importance = TRUE
#   cutoff = c(0.99, 0.01) - use strict rule converting votes to prediction

set.seed(3, sample.kind="Rounding")
random_forest_model_1 <- randomForest(factor(is_edible) ~ .,
                             data = mushroom_training_set,
                             importance = TRUE,
                             cutoff = c(0.99, 0.01))

# Call:
#   randomForest(formula = factor(is_edible) ~ ., data = mushroom_training_set,      importance = TRUE, cutoff = c(0.99, 0.01)) 
# Type of random forest: classification
# Number of trees: 500
# No. of variables tried at each split: 4
# 
# OOB estimate of  error rate: 2.69%
# Confusion matrix:
#   e    p  class.error
# e 3191  175 0.0519904932
# p    0 3132 0.0000000000
################################################################

#look at plot of random_forest_model_1 which shows shows error as function number of trees - choose 300 trees for minimum error after first rise in error

# tune mtry done manually with:
#   independent variables in mushroom_training_set[, -1],
#   dependent variable in mushroom_training_set[, 1],
#   ntreeTry = 300,
#   stepFactor = 0.5,
#   improve = 0.002,
#   trace = TRUE,
#   plot = TRUE,
#   cutoff = c(0.99, 0.01))

# produces mtry = 8
################################################################

# use mtry = 8 and train random_forest_model with:
#   dependent variable is_edible,
#   independent variables in mushroom_training_set,
#   ntree = 500,
#   mtry = 8,
#   importance = TRUE,
#   cutoff = c(0.99, 0.01)

set.seed(4, sample.kind="Rounding")
random_forest_model <-randomForest(factor(is_edible) ~ .,
                                   ntree = 500,
                                   mtry = 8,
                                   data = mushroom_training_set,
                                   importance = TRUE,
                                   cutoff = c(0.99, 0.01))

# Call:
#  randomForest(formula = factor(is_edible) ~ ., data = mushroom_training_set,      ntree = 500, mtry = 8, importance = TRUE, cutoff = c(0.99,          0.01)) 
#                Type of random forest: classification
#                      Number of trees: 500
# No. of variables tried at each split: 8
# 
#         OOB estimate of  error rate: 0.12%
# Confusion matrix:
#      e    p class.error
# e 3358    8  0.00237671
# p    0 3132  0.00000000

```

For this project, given the high cost of making a wrong prediction and eating a poisonous mushroom, the selection of output class for each observation is based upon which class has greater than 0.99 of the considered votes from all trees (note "cutoff" argument setting below). The random forest model was first trained on the data frame *mushroom_training_set* using the randomForest function with default settings of:

-   "ntree" set to 500,
-   "mtry" set to 4, and
-   "cutoff" set to (0.99, 0.01).

This model had an out-of-bag error of 2.69% and misclassified 175 edible observations as non-edible. A plot of the model error as a function of number of trees was consulted and a minimum of error was evident at about 300 trees. Tuning of mtry was performed manually using the tuneRF function. An optimal mtry of 8 resulted.

Using this tuned mtry a random forest model was trained on the data frame *mushroom_training_set* using the randomForest function with:

-   *is_edible* from *mushroom_training_set* as the dependent binary variable and positive class set to "edible",
-   the 20 other independent variables from *mushroom_training_set* as features for training,
-   "ntree" set to 500,
-   "mtry" tuned and set to optimal 8, and
-   "cutoff" set to (0.99, 0.01).

The trained model had a low 0.12% out-of-bag estimate of error with only 8 false negatives predicted (edible mushrooms predicted to be non-edible).

\pagebreak

One way to look at variable importance in the trained model involves the mean decrease accuracy. Mean decrease accuracy is an indicator of the reduction in model accuracy if that variable is removed from the model. Larger values of mean decrease accuracy indicate more important variables. Here are the model variables  in decreasing order of mean decrease accuracy:

```{r, echo=FALSE, message=FALSE, warning=FALSE, comment = NA}
#################################################################
#create data frame with two column of random forest variables and mean decrease accuracy (mda) arranged by decreasing mda
#################################################################

rf_ID_var_mda <- as.data.frame(random_forest_model$importance) %>% 
  mutate(MeanDecreaseAccuracy = round(MeanDecreaseAccuracy, 3)) %>%
  select(MeanDecreaseAccuracy) %>% 
  arrange(desc(MeanDecreaseAccuracy))

rf_ID_var_mda
```

*odor*, *spore_print_color,* *gill_color* and *stalk_surface_above_ring* had the highest mean decrease accuracies. Once again odor seems to be an important predictor for mushroom edibility.



Random forest model prediction was performed using the predict function with:

-   *random_forest_model*, and
-   *mushroom_test_set* (test data independent variables).

This produced predictions which were then compared to the actual dependent variable observations in the test data (*mushroom_test_set\$is_edible*) and confusion matrix statistics were calculated.

\pagebreak

## Results

When modeling the prediction of mushroom edibility the serious consequences of eating a poisonous mushroom argue for a very strict rule of what gets classified as edible. The rule of accepting probabilities above 0.5 as positive is not appropriate and therefore the very constrained rule of accepting probabilities over 0.99 was used. In the logistic model this was accomplished when model generated probabilities were converted to predictions. In the random forest model this was accomplished through the use of the randomForest function argument "cutoff".

**Logistic Model**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
################################################################
# test regularized logistic model on test set
# predict cv_lasso_model using x_test_glmnet
################################################################

# use strict test of greater than 0.99  for probabilities to be set to prediction of 1, if 0.99 or lower set to 0

options(digits = 3)
cv_LASSO_model_test_probabilities <- predict(cv_LASSO_model,
                                                x_test_glmnet,
                                                s = "lambda.min",
                                                type = "response")

cv_LASSO_model_test_predictions <- ifelse(cv_LASSO_model_test_probabilities > 0.99, 1, 0) %>%
  factor()

# confusion matrix calculations assuming edible is positive class
cv_LASSO_confusion_matrix <- table(cv_LASSO_model_test_predictions,
                          factor(y_test_glmnet[, 2]))

cv_LASSO_accuracy <- (cv_LASSO_confusion_matrix[4] + cv_LASSO_confusion_matrix[1])/sum(cv_LASSO_confusion_matrix[1:4])

cv_LASSO_sensitivity <- cv_LASSO_confusion_matrix[4]/(cv_LASSO_confusion_matrix[4] + cv_LASSO_confusion_matrix[2])

cv_LASSO_specificity <- cv_LASSO_confusion_matrix[1]/(cv_LASSO_confusion_matrix[1] + cv_LASSO_confusion_matrix[3])

cv_LASSO_false_positives <- cv_LASSO_confusion_matrix[3]

cv_LASSO_false_negatives <- cv_LASSO_confusion_matrix[2]
```

CV LASSO model test set prediction results:

|                 |                              |
|-----------------|------------------------------|
| Accuracy        | `r cv_LASSO_accuracy`        |
| Sensitivity     | `r cv_LASSO_sensitivity`     |
| Specificity     | `r cv_LASSO_specificity`     |
| False Positives | `r cv_LASSO_false_positives` |
| False Negatives | `r cv_LASSO_false_negatives` |

The CV LASSO model's accuracy was near perfect when predicting from the test set data and no false positive classifications were predicted. Only two observations were incorrectly classified - 2 edible mushrooms were classified as inedible (false negatives). The model's sensitivity, the ability to predict positive outcomes (edible), was 1 and it's specificity, the ability to predict negative outcomes (non-edible), was 0.997.

**Random Forest**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
################################################################
# test random forest model on test set
# predict random forest model on mushroom_test_set
################################################################

options(digits = 6)
set.seed(4, sample.kind = "Rounding")
random_forest_model_test_predictions <- predict(random_forest_model, mushroom_test_set)

# confusion matrix calculations assuming edible is positive class

rf_confusion_matrix <- table(random_forest_model_test_predictions,
                          factor(mushroom_test_set$is_edible))

rf_accuracy <- (rf_confusion_matrix[1] + rf_confusion_matrix[4])/sum(rf_confusion_matrix[1:4])

rf_sensitivity <- rf_confusion_matrix[1]/(rf_confusion_matrix[1] + rf_confusion_matrix[3])

rf_specificity <- rf_confusion_matrix[4]/(rf_confusion_matrix[4] + rf_confusion_matrix[2])

rf_false_positives <- rf_confusion_matrix[2]

rf_false_negatives <- rf_confusion_matrix[3]
```

Random Forest model test set prediction results:

|                 |                        |
|-----------------|------------------------|
| Accuracy        | `r rf_accuracy`        |
| Sensitivity     | `r rf_sensitivity`     |
| Specificity     | `r rf_specificity`     |
| False Positives | `r rf_false_positives` |
| False Negatives | `r rf_false_negatives` |

The Random Forest model's prediction from the test set data was excellent with accuracy, sensitivity and specificity all 1. The model predicted all observations correctly.

\pagebreak

## Conclusion

The goal of this project was accomplished. Two prediction models of the edibility of mushrooms were successfully constructed using the UCI Mushroom data set. One model utilized the random forest algorithm, the other, regularized logistic regression. Both models were very accurate predictors of the test set data. The random forest model predicted perfectly on the test set - no misclassifications. The regularized logistic model correctly predicted 99.9% of the test data and only misclassified 2 edible mushrooms as non-edible. There were no classifications of non-edible mushrooms as edible in either model which would of course be a serious problem.

One of the limitations of this work is that little was done to determine the role of individual variables as predictors. The random forest model produced the mean decrease accuracies of the variables and these were presented. Coefficients from the regularized logistic model were presented but without understanding the significance of each. Better understanding of the role of each variable in prediction is an area for potential future work which may lead to models with fewer features.

## References

1.  <https://archive-beta.ics.uci.edu/ml/datasets/mushroom>
2.  Quote from website page in reference 1.
3.  Cramér, Harald. 1946. *Mathematical Methods of Statistics.* Princeton: Princeton University Press, Page 282 (Chapter 21. The two dimensional case).
4.  A nominal variable is a type of categorical variable where the information in the variable's levels have no numeric value.

\pagebreak

![page 1](Appendix-p-1%20Table-of-Mushroom-Variables-with-Levels.png) ![page 2](Appendix-p-2%20Table-of-Mushroom-Variables-with-Levels.png) ![page 3](Appendix-p-3%20Table-of-Mushroom-Variables-with-Levels.png)
